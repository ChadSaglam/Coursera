{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3729bc4-a057-4a6e-8832-d38f1a77954f",
   "metadata": {},
   "source": [
    "# Web Scraping Job Vacancies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0753f9c-02be-4034-b54b-65c8f6529253",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this project, we'll build a web scraper to extract job listings from a popular job search platform. We'll extract job titles, companies, locations, job descriptions, and other relevant information.\n",
    "\n",
    "Here are the main steps we'll follow in this project:\n",
    "\n",
    "1. Setup our development environment\n",
    "2. Understand the basics of web scraping\n",
    "3. Analyze the website structure of our job search platform\n",
    "4. Write the Python code to extract job data from our job search platform\n",
    "5. Save the data to a CSV file\n",
    "6. Test our web scraper and refine our code as needed\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Before starting this project, you should have some basic knowledge of Python programming and HTML structure. In addition, you may want to use the following packages in your Python environment:\n",
    "\n",
    "- requests\n",
    "- BeautifulSoup\n",
    "- csv\n",
    "- datetime\n",
    "\n",
    "These packages should already be installed in Coursera's Jupyter Notebook environment, however if you'd like to install additional packages that are not included in this environment or are working off platform you can install additional packages using `!pip install packagename` within a notebook cell such as:\n",
    "\n",
    "- `!pip install requests`\n",
    "- `!pip install BeautifulSoup`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "473995d1-10e8-4565-9496-759d3601ef42",
   "metadata": {},
   "source": [
    "## Step 1: Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c30ac9c4-7693-445d-b29b-f7990e530495",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc7fbdb-61f0-4622-b6d0-cee415f6c417",
   "metadata": {},
   "source": [
    "## Step 2: Analyze the Website Structure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e9b7bb-a674-49f7-aa6d-bfd05d6ea344",
   "metadata": {},
   "source": [
    "1. Visit the URL: https://www.jobs.ch/en/vacancies/?location=zurich&term=data%20analyst.\n",
    "2. Inspect the page (right-click → Inspect) to understand the HTML structure.\n",
    "3. Identify the tags and classes for job titles, companies, locations, and descriptions.\n",
    "\n",
    "For jobs.ch:\n",
    "\n",
    "1. Job titles are usually in \"h2\" tags.\n",
    "2. Company names and locations are often in \"span\" or \"div\" tags.\n",
    "3. Job descriptions might be in \"div\" tags with specific classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c00cf2-8688-4fea-82bc-50cc486877e9",
   "metadata": {},
   "source": [
    "## Step 3: Write Python Code to Extract Job Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d4ce27-02a9-44fa-a580-95690117edee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the URL of the job search page\n",
    "url = \"https://www.jobs.ch/en/vacancies/?term=data%20analyst\"\n",
    "\n",
    "# Send a GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    print(\"Successfully fetched the webpage!\")\n",
    "else:\n",
    "    print(f\"Failed to fetch the webpage. Status code: {response.status_code}\")\n",
    "\n",
    "# Parse the HTML content using BeautifulSoup\n",
    "soup = BeautifulSoup(response.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41493b21-c5ec-4333-adbc-93ab2896bc02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all job listings on the page\n",
    "job_listings = soup.find_all('div', class_='d_flex bg-c_brand.01 bdr_r16 flex-d_column h_100% p_s16 pos_relative')\n",
    "#print(job_listings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f09c87-326a-482d-b15e-5f090adf458f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a list to store job data\n",
    "jobs = []\n",
    "\n",
    "# Loop through each job listing and extract details\n",
    "for job in job_listings:\n",
    "    # Extract job title\n",
    "    title = job.find('span', class_='c_gray.90 mb_s4 mr_s8 textStyle_h4 wb_break-word ov_hidden tov_ellipsis d_-webkit-box hy_auto ov-wrap_break-word white-space_normal word-wrap_break-word box-orient_vertical lc_4').text.strip()\n",
    "    \n",
    "    # Extract company name\n",
    "    company = job.find('p', class_='mb_s12 lastOfType:mb_s0 textStyle_p2 c_gray.90').text.strip()\n",
    "    \n",
    "    # Extract job location\n",
    "    location = job.find('p', class_='mb_s12 lastOfType:mb_s0 textStyle_p2').text.strip()\n",
    "    \n",
    "    # Extract job description (if available)\n",
    "    #description_tag = job.find('div', class_='vacancy-card__description')\n",
    "    #description = description_tag.text.strip() if description_tag else \"No description available\"\n",
    "    \n",
    "    # Append the job details to the list\n",
    "    jobs.append({\n",
    "        'Title': title,\n",
    "        'Company': company,\n",
    "        'Location': location\n",
    "        #'Description': description\n",
    "    })\n",
    "\n",
    "# Print the first job to verify\n",
    "print(jobs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e96078b0-8496-4feb-9e5f-e06109faf6b0",
   "metadata": {},
   "source": [
    "## Step 4: Save the Data to a CSV File\n",
    "\n",
    "Now that we’ve extracted the data, let’s save it to a CSV file for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a35719f-8c27-45d8-8b82-5813e82bf926",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the filename with a timestamp\n",
    "filename = f\"job_postings_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
    "\n",
    "# Write the data to a CSV file\n",
    "with open(filename, mode='w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.DictWriter(file, fieldnames=['Title', 'Company', 'Location'])\n",
    "    writer.writeheader()\n",
    "    writer.writerows(jobs)\n",
    "\n",
    "print(f\"Data saved to {filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e956a4-5fd9-4247-861a-962d5eaa0968",
   "metadata": {},
   "source": [
    "## Step 5: Test and Refine the Code\n",
    "\n",
    "1. Run the code and check the output CSV file to ensure it contains the expected data.\n",
    "2. If the website has pagination (multiple pages of job listings), modify the code to loop through all pages. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b79e76cb-d79b-413a-bc62-5a6137847dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = \"https://www.jobs.ch/en/vacancies/?term=data%20analyst\"\n",
    "jobs = []\n",
    "\n",
    "# Loop through the first 5 pages (adjust as needed)\n",
    "for page in range(1, 6):  # Scrape pages 1 to 5\n",
    "    url = base_url + str(page)\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    job_listings = soup.find_all('div', class_='d_flex bg-c_brand.01 bdr_r16 flex-d_column h_100% p_s16 pos_relative')\n",
    "    \n",
    "    for job in job_listings:\n",
    "        title = job.find('span', class_='c_gray.90 mb_s4 mr_s8 textStyle_h4 wb_break-word ov_hidden tov_ellipsis d_-webkit-box hy_auto ov-wrap_break-word white-space_normal word-wrap_break-word box-orient_vertical lc_4').text.strip()\n",
    "        company = job.find('p', class_='mb_s12 lastOfType:mb_s0 textStyle_p2 c_gray.90').text.strip()\n",
    "        location = job.find('p', class_='mb_s12 lastOfType:mb_s0 textStyle_p2').text.strip()\n",
    "        \n",
    "        jobs.append({\n",
    "            'Title': title,\n",
    "            'Company': company,\n",
    "            'Location': location\n",
    "        })\n",
    "\n",
    "# Save the updated data to a CSV file\n",
    "filename = f\"job_postings_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
    "with open(filename, mode='w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.DictWriter(file, fieldnames=['Title', 'Company', 'Location'])\n",
    "    writer.writeheader()\n",
    "    writer.writerows(jobs)\n",
    "\n",
    "print(f\"Data saved to {filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9abcf3cf-6fd7-4b49-a0fd-8ea800f5a7c3",
   "metadata": {},
   "source": [
    "## Step 6: Analyze and Visualize the\n",
    "\n",
    "Once the data is saved, you can analyze it using libraries like pandas and matplotlib. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51526ce0-699c-4ff3-b35c-235cf1acc36c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
